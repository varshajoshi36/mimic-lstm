{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import argparse, re, random, json\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f01e099afd0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/soe/dcjenkin/workspace/venv/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "n_input = 14   # (OH encoded vector of length 13, value)\n",
    "\n",
    "n_steps = 100  #100 timesteps per batch\n",
    "\n",
    "n_hidden = 100  # hidden layer num of features\n",
    "\n",
    "n_classes = 1  # Mortality in hospital.  The network will put out one value between 0 and 1 at each timestep indicating the prediction whether the patient has lived or died.  (0 for lived, 1 for died)\n",
    "\n",
    "batch_size = 30\n",
    "\n",
    "# X_lengths = tf.placeholder(tf.int32, [None], name='X_lengths')  #feed in the unpadded sequence length for each batch\n",
    "\n",
    "# tf Graph input (X is a 3D array of inuts (number of patients x number of measurements x 14 for OH vector, and value))\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_input], name='X')\n",
    "# y is a 1D array of labels shape = (number of patients). Here for a given patient the label is the same at each time step since it just indicates whether that patient died in the hospital.\n",
    "y = tf.placeholder(tf.float32, [None], name='y')\n",
    "y2 = tf.reshape(y, shape = (batch_size,-1))\n",
    "\n",
    "#Weights and Biases to map hidden state to n_classes predictions\n",
    "w1 = tf.Variable(tf.random_normal([n_hidden, n_classes]), name='w1')\n",
    "b1 = tf.Variable(tf.random_normal([n_classes]), name='w2')\n",
    "\n",
    "\n",
    "def length(sequence):\n",
    "    #returns a vector of sequence lengths for each patient within the batch segment\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(sequence), 2))\n",
    "    length = tf.reduce_sum(used, 1)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length\n",
    "\n",
    "X_l = length(X)\n",
    "\n",
    "# Define a lstm cell with tensorflow\n",
    "lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=False)\n",
    "\n",
    "state = tf.Variable(lstm_cell.zero_state(batch_size, tf.float32), trainable=False)\n",
    "\n",
    "outputs, states = tf.nn.dynamic_rnn(\n",
    "        cell=lstm_cell,\n",
    "        dtype=tf.float32,\n",
    "        sequence_length=X_l,\n",
    "        inputs=X,\n",
    "        initial_state=state)\n",
    "\n",
    "state_op = tf.assign(state, states)\n",
    "\n",
    "\n",
    "def last_relevant(output, length):\n",
    "    #returns outputs for each patient at the indexes given in length\n",
    "    batch_size = tf.shape(output)[0]\n",
    "    max_length = tf.shape(output)[1]\n",
    "    out_size = int(output.get_shape()[2])\n",
    "    index = tf.range(0, batch_size) * max_length + (length - 1)\n",
    "    flat = tf.reshape(output, [-1, out_size])\n",
    "    relevant = tf.gather(flat, index)\n",
    "    return relevant\n",
    "\n",
    "\n",
    "# Remove patients from batch that have no measurements for that batch (ie the length of the sequence is zero for that patient for that batch).\n",
    "# mask = np.array([False, True])\n",
    "mask = X_l > 0\n",
    "outputs2 = tf.boolean_mask(outputs, mask)\n",
    "X_l2 = tf.boolean_mask(X_l, mask)\n",
    "y3 = tf.boolean_mask(y2, mask)\n",
    "masksum = tf.reduce_sum(tf.cast(mask, tf.float32))\n",
    "\n",
    "# outputs2 = outputs[1]\n",
    "# X_l \n",
    "\n",
    "rel = last_relevant(outputs2, X_l2)\n",
    "\n",
    "pred1 = tf.matmul(rel, w1)\n",
    "\n",
    "pred2 = tf.add(pred1,b1)\n",
    "\n",
    "sigmoid_pred = tf.sigmoid(pred2)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(targets = y3,logits = pred2, pos_weight= 1), name='cost')\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost, name='optimizer')\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_filenames = []\n",
    "test_filenames = []\n",
    "# val_filenames = []\n",
    "\n",
    "for i in np.arange(70):\n",
    "    train_filenames.append('../data/xN' + str(i)+ '.tfrecord')\n",
    "    \n",
    "# for i in np.arange(10):\n",
    "#     train_filenames.append('../data/xC' + str(i+90)+ '.tfrecord')\n",
    "\n",
    "for i in np.arange(10):\n",
    "    test_filenames.append('../data/xN' + str(i+70) + '.tfrecord')\n",
    "    \n",
    "# for i in np.arange(20):\n",
    "#     val_filenames.append('../data/xC' + str(i+80) + '.tfrecord')\n",
    "\n",
    "train_dataset = tf.data.TFRecordDataset(train_filenames)\n",
    "   \n",
    "test_dataset = tf.data.TFRecordDataset(test_filenames)\n",
    "    \n",
    "# val_dataset = tf.data.TFRecordDataset(val_filenames)\n",
    "\n",
    "# Transforms a scalar string `example_proto` into a pair of a scalar string and\n",
    "# a scalar integer, representing an input and its label, respectively.  tlen is the number of measurements for the patient.\n",
    "def _parse_function(example_proto):\n",
    "  features = {\"train/pat_n\": tf.FixedLenFeature((), tf.string, default_value=\"\"),\n",
    "              \"train/label\": tf.FixedLenFeature((), tf.int64, default_value=0),\n",
    "              \"train/tlen\": tf.FixedLenFeature((), tf.int64, default_value=0)}\n",
    "  parsed_features = tf.parse_single_example(example_proto, features)\n",
    "  return parsed_features[\"train/pat_n\"], parsed_features[\"train/label\"], parsed_features[\"train/tlen\"]\n",
    "\n",
    "train_dataset = train_dataset.map(_parse_function, num_parallel_calls=10)\n",
    "test_dataset = test_dataset.map(_parse_function, num_parallel_calls=10)\n",
    "# val_dataset = val_dataset.map(_parse_function, num_parallel_calls=10)\n",
    "\n",
    "def _parse_function2(patd_str, label, tlen):\n",
    "    #decodes strings and converts to floats and integers\n",
    "    patd = tf.decode_raw(patd_str, out_type = tf.float64)\n",
    "    patd = tf.to_float(patd)\n",
    "    label = tf.to_int32(label)\n",
    "    tlen = tf.to_int32(tlen)\n",
    "    #finds the number of groups.  Each group contains n_steps = 100 measurements per patient (eg if there are 637 measurements for a patient that would form 7 groups).\n",
    "    numgroup = tf.reduce_max([tf.to_int32(tf.ceil(tlen/100))-1, 0])\n",
    "    #generates labels for each group indicating mortality at the end of that group.  Only the final group can be non-zero as the patients are still alive if measurements are still being taken.\n",
    "    labellist = tf.zeros([numgroup], tf.int32)\n",
    "    labellist = tf.concat([labellist, [label]], 0)\n",
    "    #Reshape into 2D array.  There are 15 values at each time step (One for each feature type).\n",
    "    patd = tf.reshape(patd, [-1,15])\n",
    "    #take all the feature types except for the time which is not used here.  (time is at index 13).\n",
    "    patd = tf.gather(patd, indices = [0,1,2,3,4,5,6,7,8,9,10,11,12,14], axis = 1)\n",
    "    return patd, label, tlen, numgroup, labellist\n",
    "\n",
    "train_dataset = train_dataset.map(_parse_function2, num_parallel_calls=10)\n",
    "test_dataset = test_dataset.map(_parse_function2, num_parallel_calls=10)\n",
    "\n",
    "#randomize the order of training examples\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1500)\n",
    "train_dataset = train_dataset.repeat()\n",
    "# test_dataset = test_dataset.repeat()\n",
    "\n",
    "#batch together mulitple patient's data.  The ends are padded with zeros to a value which is greater than the maximum number of measurements per patient and is divisible by n_steps)\n",
    "train_dataset = train_dataset.padded_batch(batch_size, padded_shapes=([300000,14],[],[],[],[None]))\n",
    "test_dataset = test_dataset.padded_batch(batch_size, padded_shapes=([300000,14],[],[],[],[None]))\n",
    "\n",
    "\n",
    "def _parse_function3(patt, label, tlen, numgroup, labellist):\n",
    "    #find the max number of measurements for patients in the batch.\n",
    "    mlen = tf.reduce_max(tlen)\n",
    "    #round up to nearest 100 and take only the values up to this index.  (All values after rmlen should be zeros from the padding)\n",
    "    rmlen = tf.to_int32(tf.ceil(mlen/100)*100)\n",
    "    patt = patt[:,0:rmlen]\n",
    "    #split up into groups of 100 measurements each\n",
    "    patt = tf.reshape(patt, [batch_size,-1,100,14])\n",
    "    #transpose to match format for rnn.\n",
    "    patt = tf.transpose(patt, perm=[1, 0, 2, 3]) #(number of groups, batch size, n_steps, number of feature types)\n",
    "    labellist = tf.transpose(labellist, perm=[1,0])\n",
    "#     numsplits = tf.to_int32(rmlen/100)\n",
    "#     numsplits = tf.to_int32(tf.ceil(mlen/100\n",
    "\n",
    "    return patt, label, tlen, numgroup, labellist\n",
    "\n",
    "train_dataset = train_dataset.map(_parse_function3, num_parallel_calls=10) \n",
    "test_dataset = test_dataset.map(_parse_function3, num_parallel_calls=10) \n",
    "\n",
    "train_iterator = train_dataset.make_one_shot_iterator()\n",
    "train_next_element = train_iterator.get_next()\n",
    "\n",
    "test_iterator = test_dataset.make_initializable_iterator()\n",
    "test_next_element = test_iterator.get_next()\n",
    "\n",
    "train_dataset = train_dataset.prefetch(5)\n",
    "test_dataset = test_dataset.prefetch(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c10b16b66078>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                     \u001b[0mtest_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_next_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;31m#                 print(sess.run(state))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/soe/dcjenkin/workspace/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/soe/dcjenkin/workspace/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/soe/dcjenkin/workspace/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/soe/dcjenkin/workspace/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/soe/dcjenkin/workspace/venv/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    AUC = []\n",
    "    m = 0\n",
    "    for i in range(6000):\n",
    "        print(i)\n",
    "#         print(sess.run(state))\n",
    "        train_value = sess.run(train_next_element)\n",
    "        #initializes hidden/cell states to zero at the beginning of each patient batch.\n",
    "        sess.run(tf.variables_initializer([state]))\n",
    "#         print(train_value[1])\n",
    "#         print(sess.run(state))\n",
    "\n",
    "        for j in np.arange(np.shape(train_value[0])[0]):\n",
    "            #take the jth group's input and labels to feed into the LSTM.\n",
    "            batch_x, batch_y = train_value[0][j], train_value[4][j]\n",
    "\n",
    "            feed = {X: batch_x, y: batch_y}\n",
    "\n",
    "            #run the session.  (optimizer will update parameters).  State op will update the hidden state and cell state to the values at the end of the previous group.\n",
    "            result = sess.run([optimizer, state_op], feed_dict=feed)\n",
    "\n",
    "#             print('train cost:' + str(result[0]))\n",
    "#             print('Pred:' + str(result[1]))\n",
    "#             print('label:' + str(result[2]))\n",
    "#             AUC = roc_auc_score(result[2].flatten(), result[1].flatten())\n",
    "#             print('AUC:' + str(AUC))\n",
    "             \n",
    "        if i%300 == 0:\n",
    "            #save_path = saver.save(sess, \"../models/model5\" + str(m) + \".ckpt\")\n",
    "            #m +=1\n",
    "            #create arrays for the prediction outputs and labels.\n",
    "            preds = np.array([])\n",
    "            labs = np.array([])\n",
    "            #initialize the test iterator to start at the beginning\n",
    "            sess.run(test_iterator.initializer)\n",
    "            while True:\n",
    "                try:\n",
    "                    test_value = sess.run(test_next_element)\n",
    "    #                 print(sess.run(state))\n",
    "                    sess.run(tf.variables_initializer([state]))\n",
    "    #                 print(sess.run(state))\n",
    "    #                 print(test_value[1])\n",
    "\n",
    "                    for k in np.arange(np.shape(test_value[0])[0]):\n",
    "                        #take the k'th group's input and labels to feed into the LSTM.\n",
    "                        batch_x, batch_y = test_value[0][k], test_value[4][k]\n",
    "\n",
    "                        feed = {X: batch_x, y: batch_y}\n",
    "\n",
    "    #                     print(sess.run(state))\n",
    "                        result = sess.run([sigmoid_pred, y3, state_op], feed_dict=feed)\n",
    "    #                     print(sess.run(state))\n",
    "    #                     print('test cost:' + str(result[0]))\n",
    "    #                     print('Pred:' + str(result[1]))\n",
    "    #                     print('label:' + str(result[2]))\n",
    "                        #append the predictions and labels to the arrays\n",
    "                        preds = np.append(preds, result[0].flatten())\n",
    "                        labs = np.append(labs, result[1].flatten())\n",
    "                except tf.errors.InvalidArgumentError:\n",
    "                    break\n",
    "            #Compute and print the area under receiver operator curve for predictions and labels\n",
    "            AUC.append(roc_auc_score(labs, preds))\n",
    "            print('Test AUC:' + str(AUC))\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
